import os
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

class FeedbackAgent:
    def __init__(self):
        self.llm = ChatOpenAI(
            model="nvidia/nemotron-3-nano-30b-a3b:free", 
            temperature=0.1, # Low temperature for critical analysis
            base_url="https://openrouter.ai/api/v1",
            api_key=os.getenv("OPENROUTER_API_KEY")
        )

    def critique_and_refine(self, original_response, context=""):
        """
        Critiques the response for medical safety, hallucinations, and clarity.
        If unsafe or inaccurate, provides a refined version.
        """
        
        system_prompt = """You are a Senior Medical Safety Consultant (AI Auditor). 
        Your job is to review the following medical advice generated by an AI assistant.
        
        Check for:
        1. **Safety**: Are there any dangerous suggestions?
        2. **Hallucinations**: Does it mention made-up treatments?
        3. **Clarity**: Is it easy for a patient to understand?

        IF the advice is safe, REFORMAT it into a PATIENT-FRIENDLY Structured Format:
        
        ### [Condition Name] Analysis
        
        | Laboratory Test | Patient's Result | Status | Interpretation |
        |-----------------|------------------|--------|----------------|
        | [Name] | [Value] | [High/Low/Normal] | [Brief Meaning] |
        
        ### Action Plan
        - **[Action 1]**: [Reasoning]
        - **[Action 2]**: [Reasoning]
        
        ### Safety Disclaimer
        [Required Disclaimer]

        If the advice is dangerous, REWRITE it completely using this format.
        
        Output format:
        [CRITIQUE]: Brief analysis (1 sentence).
        [REFINED_RESPONSE]: The final formatted response.
        """

        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", f"Context: {context}\n\nAI Response to Review:\n{original_response}")
        ])

        chain = prompt | self.llm | StrOutputParser()
        
        try:
            result = chain.invoke({})
            return result
        except Exception as e:
            return f"[ERROR] Feedback Agent Failed: {e}\n\n[REFINED_RESPONSE]\n{original_response}"

if __name__ == "__main__":
    # Test
    agent = FeedbackAgent()
    print(agent.critique_and_refine("You should drink 10 liters of water to flush kidneys.", "CKD Patient"))
